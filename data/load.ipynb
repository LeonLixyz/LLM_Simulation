{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyreadstat\n",
    "data_name = \"ATP_W82\"\n",
    "df_raw, meta = pyreadstat.read_sav(f\"{data_name}.sav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map Variable Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, meta = pyreadstat.read_sav(f\"{data_name}.sav\")\n",
    "\n",
    "# Function to replace values with labels\n",
    "def replace_with_labels(series, value_labels):\n",
    "    return series.map(value_labels).fillna(series)\n",
    "\n",
    "# Create a dictionary to map original column names to labels\n",
    "column_label_dict = dict(zip(meta.column_names, meta.column_labels))\n",
    "\n",
    "# Create a reverse mapping from labels to original names\n",
    "reverse_column_dict = {v: k for k, v in column_label_dict.items()}\n",
    "\n",
    "# Rename columns using the labels\n",
    "df.rename(columns=column_label_dict, inplace=True)\n",
    "\n",
    "# Apply value labels to all columns that have them\n",
    "for column in df.columns:\n",
    "    original_column_name = reverse_column_dict.get(column, column)\n",
    "    if original_column_name in meta.variable_value_labels:\n",
    "        df[column] = replace_with_labels(df[column], meta.variable_value_labels[original_column_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-NaN indices saved to /user/al4263/Simulate/Simulations/Pew_Research/ATP_W82/non_nan_indices.json\n",
      "\n",
      "Question ID: GAP21Q3_W82\n",
      "Number of non-NaN responses: 2596\n",
      "Sample indices: [0, 1, 2, 3, 4]...\n",
      "\n",
      "Question ID: GAP21Q5_b_W82\n",
      "Number of non-NaN responses: 2596\n",
      "Sample indices: [0, 1, 2, 3, 4]...\n",
      "\n",
      "Question ID: GAP21Q5_a_W82\n",
      "Number of non-NaN responses: 2596\n",
      "Sample indices: [0, 1, 2, 3, 4]...\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load the context file\n",
    "with open(f'/user/al4263/Simulate/Simulations/Pew_Research/{data_name}/context.json', 'r') as f:\n",
    "    context_data = json.load(f)\n",
    "\n",
    "# Initialize a dictionary to store non-NaN indices for each question\n",
    "non_nan_indices = {}\n",
    "\n",
    "# Process each topic in the context\n",
    "for topic, topic_data in context_data.items():\n",
    "    question_id = topic_data['question_id']\n",
    "    \n",
    "    # Find the index of the question_id in df_raw\n",
    "    column_index = df_raw.columns.get_loc(question_id)\n",
    "    \n",
    "    # Get the corresponding processed column name from df\n",
    "    processed_column_name = df.columns[column_index]\n",
    "    \n",
    "    # Extract the column from df\n",
    "    extracted_column = df[processed_column_name]\n",
    "    \n",
    "    # Get indices of non-NaN values\n",
    "    non_nan_mask = ~extracted_column.isna()\n",
    "    non_nan_indices[question_id] = non_nan_mask[non_nan_mask].index.tolist()\n",
    "\n",
    "# Save the non-NaN indices to a JSON file\n",
    "output_dir = f\"/user/al4263/Simulate/Simulations/Pew_Research/{data_name}\"\n",
    "output_file = os.path.join(output_dir, \"non_nan_indices.json\")\n",
    "\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(non_nan_indices, f, indent=2)\n",
    "\n",
    "print(f\"Non-NaN indices saved to {output_file}\")\n",
    "\n",
    "# Print a sample of the results\n",
    "for question_id, indices in list(non_nan_indices.items())[:3]:  # Show first 3 items\n",
    "    print(f\"\\nQuestion ID: {question_id}\")\n",
    "    print(f\"Number of non-NaN responses: {len(indices)}\")\n",
    "    print(f\"Sample indices: {indices[:5]}...\")  # Show first 5 indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Human Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to /user/al4263/Simulate/Simulations/Pew_Research/ATP_W116/human_results/human_results.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Function to convert non-serializable objects to strings\n",
    "def json_serial(obj):\n",
    "    if isinstance(obj, (datetime, pd.Timestamp)):\n",
    "        return obj.isoformat()\n",
    "    raise TypeError(f\"Type {type(obj)} not serializable\")\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "output_dir = f\"/user/al4263/Simulate/Simulations/Pew_Research/{data_name}/human_results\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Initialize a dictionary to store all results\n",
    "all_results = {}\n",
    "\n",
    "# Find the index of GAP21Q1_W82\n",
    "start_index = df_raw.columns.get_loc(\"SATIS_W116\")\n",
    "\n",
    "# Iterate through columns in df_raw starting from GAP21Q1_W82\n",
    "for column_raw in df_raw.columns[start_index:]:\n",
    "    # Get the corresponding column name in df\n",
    "    df_column_name = df.columns[df_raw.columns.get_loc(column_raw)]\n",
    "    \n",
    "    # Perform the count on df\n",
    "    counts = df[df_column_name].value_counts().to_dict()\n",
    "    \n",
    "    # Convert any non-serializable objects in counts to strings\n",
    "    counts = {str(k): v for k, v in counts.items()}\n",
    "    \n",
    "    # Store the results\n",
    "    all_results[column_raw] = {\n",
    "        \"id\": column_raw,\n",
    "        \"question\": df_column_name,\n",
    "        \"counts\": counts\n",
    "    }\n",
    "\n",
    "# Save the results to a JSON file\n",
    "output_file = os.path.join(output_dir, \"human_results.json\")\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(all_results, f, indent=2, default=json_serial)\n",
    "\n",
    "print(f\"Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected results saved to /user/al4263/Simulate/Simulations/Pew_Research/ATP_W82/human_results/selected_human_results.json\n",
      "Number of selected questions: 21\n"
     ]
    }
   ],
   "source": [
    "def json_serial(obj):\n",
    "    if isinstance(obj, (datetime, pd.Timestamp)):\n",
    "        return obj.isoformat()\n",
    "    raise TypeError(f\"Type {type(obj)} not serializable\")\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "output_dir = f\"/user/al4263/Simulate/Simulations/Pew_Research/{data_name}/human_results\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load the context file\n",
    "with open(f'/user/al4263/Simulate/Simulations/Pew_Research/{data_name}/context.json', 'r') as f:\n",
    "    context_data = json.load(f)\n",
    "\n",
    "# Extract question IDs from context\n",
    "context_question_ids = [topic_data['question_id'] for topic_data in context_data.values()]\n",
    "\n",
    "# Initialize a dictionary to store selected results\n",
    "selected_results = {}\n",
    "\n",
    "# Iterate through columns in df_raw\n",
    "for column_raw in df_raw.columns:\n",
    "    if column_raw in context_question_ids:\n",
    "        # Get the corresponding column name in df\n",
    "        df_column_name = df.columns[df_raw.columns.get_loc(column_raw)]\n",
    "        \n",
    "        # Perform the count on df\n",
    "        counts = df[df_column_name].value_counts()\n",
    "        \n",
    "        # Calculate percentages\n",
    "        total_responses = counts.sum()\n",
    "        percentages = (counts / total_responses * 100).round(2)\n",
    "        \n",
    "        # Sort counts and percentages in descending order\n",
    "        counts_sorted = counts.sort_values(ascending=False)\n",
    "        percentages_sorted = percentages.sort_values(ascending=False)\n",
    "        \n",
    "        # Convert to dictionaries\n",
    "        counts_dict = counts_sorted.to_dict()\n",
    "        percentages_dict = percentages_sorted.to_dict()\n",
    "        \n",
    "        # Store the results\n",
    "        selected_results[column_raw] = {\n",
    "            \"id\": column_raw,\n",
    "            \"question\": df_column_name,\n",
    "            \"results\": {\n",
    "                \"count\": counts_dict,\n",
    "                \"percentage\": percentages_dict\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Save the results to a JSON file\n",
    "output_file = os.path.join(output_dir, \"selected_human_results.json\")\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(selected_results, f, indent=2, default=json_serial)\n",
    "\n",
    "print(f\"Selected results saved to {output_file}\")\n",
    "print(f\"Number of selected questions: {len(selected_results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Persona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 100 personas...\n",
      "Created 200 personas...\n",
      "Created 300 personas...\n",
      "Created 400 personas...\n",
      "Created 500 personas...\n",
      "Created 600 personas...\n",
      "Created 700 personas...\n",
      "Created 800 personas...\n",
      "Created 900 personas...\n",
      "Created 1000 personas...\n",
      "Created 1100 personas...\n",
      "Created 1200 personas...\n",
      "Created 1300 personas...\n",
      "Created 1400 personas...\n",
      "Created 1500 personas...\n",
      "Created 1600 personas...\n",
      "Created 1700 personas...\n",
      "Created 1800 personas...\n",
      "Created 1900 personas...\n",
      "Created 2000 personas...\n",
      "Created 2100 personas...\n",
      "Created 2200 personas...\n",
      "Created 2300 personas...\n",
      "Created 2400 personas...\n",
      "Created 2500 personas...\n",
      "Created 2600 personas...\n",
      "Created 2700 personas...\n",
      "Created 2800 personas...\n",
      "Created 2900 personas...\n",
      "Created 3000 personas...\n",
      "Created 3100 personas...\n",
      "Created 3200 personas...\n",
      "Created 3300 personas...\n",
      "Created 3400 personas...\n",
      "Created 3500 personas...\n",
      "Created 3600 personas...\n",
      "Created 3700 personas...\n",
      "Created 3800 personas...\n",
      "Created 3900 personas...\n",
      "Created 4000 personas...\n",
      "Created 4100 personas...\n",
      "Created 4200 personas...\n",
      "Created 4300 personas...\n",
      "Created 4400 personas...\n",
      "Created 4500 personas...\n",
      "Created 4600 personas...\n",
      "Created 4700 personas...\n",
      "Created 4800 personas...\n",
      "Created 4900 personas...\n",
      "Created 5000 personas...\n",
      "Created a total of 5098 personas in /user/al4263/Simulate/Pew_Research/ATP_W116/persona_meta\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Define the base directory and create the persona_meta folder\n",
    "base_dir = f\"/user/al4263/Simulate/Pew_Research/{data_name}\"\n",
    "persona_dir = os.path.join(base_dir, \"persona_meta\")\n",
    "os.makedirs(persona_dir, exist_ok=True)\n",
    "\n",
    "# Find the start and end indices for the columns we want\n",
    "start_col = df.columns.get_loc(\"Metropolitan area indicator\")\n",
    "end_col = df.columns.get_loc(\"Income tier 3-way\")\n",
    "\n",
    "# Select the columns we want\n",
    "selected_columns = df.columns[start_col:end_col+1]\n",
    "\n",
    "# Counter for personas\n",
    "persona_count = 0\n",
    "\n",
    "# Iterate through each row in the dataframe\n",
    "for index, row in df.iterrows():\n",
    "    persona = {}\n",
    "    for col in selected_columns:\n",
    "        # Convert to string to ensure JSON serialization\n",
    "        persona[col] = str(row[col])\n",
    "    \n",
    "    # Create the filename for this persona\n",
    "    filename = os.path.join(persona_dir, f\"persona_{persona_count}.json\")\n",
    "    \n",
    "    # Save this persona to a JSON file\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump({\"persona\": persona}, f, indent=2)\n",
    "    \n",
    "    persona_count += 1\n",
    "\n",
    "    # Print progress every 100 personas\n",
    "    if persona_count % 100 == 0:\n",
    "        print(f\"Created {persona_count} personas...\")\n",
    "\n",
    "print(f\"Created a total of {persona_count} personas in {persona_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
