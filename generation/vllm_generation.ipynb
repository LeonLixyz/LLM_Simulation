{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user/al4263/.conda/envs/llm_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-09-22 21:14:03,519\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-22 21:14:06 config.py:729] Defaulting to use mp for distributed inference\n",
      "WARNING 09-22 21:14:06 arg_utils.py:766] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 09-22 21:14:06 config.py:820] Chunked prefill is enabled with max_num_batched_tokens=512.\n",
      "INFO 09-22 21:14:06 llm_engine.py:174] Initializing an LLM engine (v0.5.4) with config: model='meta-llama/Meta-Llama-3.1-70B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-70B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir='/shared/share_mala/Leon/huggingface/cache', load_format=LoadFormat.AUTO, tensor_parallel_size=8, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-70B-Instruct, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "WARNING 09-22 21:14:06 multiproc_gpu_executor.py:59] Reducing Torch parallelism from 128 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-22 21:14:06 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1725755)\u001b[0;0m INFO 09-22 21:14:06 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1725754)\u001b[0;0m INFO 09-22 21:14:06 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1725758)\u001b[0;0m INFO 09-22 21:14:06 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1725760)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1725756)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1725759)\u001b[0;0m INFO 09-22 21:14:06 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 09-22 21:14:06 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 09-22 21:14:06 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1725757)\u001b[0;0m INFO 09-22 21:14:06 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 09-22 21:14:11 utils.py:841] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1725754)\u001b[0;0m INFO 09-22 21:14:11 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1725760)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1725758)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1725757)\u001b[0;0m INFO 09-22 21:14:11 utils.py:841] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1725755)\u001b[0;0m INFO 09-22 21:14:11 utils.py:841] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1725756)\u001b[0;0m INFO 09-22 21:14:11 utils.py:841] Found nccl from library libnccl.so.2\n",
      "INFO 09-22 21:14:11 utils.py:841] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1725759)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1725754)\u001b[0;0m INFO 09-22 21:14:11 utils.py:841] Found nccl from library libnccl.so.2\n",
      "INFO 09-22 21:14:11 utils.py:841] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1725760)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1725758)\u001b[0;0m INFO 09-22 21:14:11 utils.py:841] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1725757)\u001b[0;0m INFO 09-22 21:14:11 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 09-22 21:14:11 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 09-22 21:14:11 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1725755)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1725756)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1725759)\u001b[0;0m INFO 09-22 21:14:11 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 09-22 21:14:11 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 09-22 21:14:11 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 09-22 21:14:11 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 09-22 21:14:14 custom_all_reduce_utils.py:234] reading GPU P2P access cache from /user/al4263/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1725759)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1725754)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1725756)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1725757)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1725758)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1725755)\u001b[0;0m INFO 09-22 21:14:14 custom_all_reduce_utils.py:234] reading GPU P2P access cache from /user/al4263/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 09-22 21:14:14 custom_all_reduce_utils.py:234] reading GPU P2P access cache from /user/al4263/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 09-22 21:14:14 custom_all_reduce_utils.py:234] reading GPU P2P access cache from /user/al4263/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 09-22 21:14:14 custom_all_reduce_utils.py:234] reading GPU P2P access cache from /user/al4263/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 09-22 21:14:14 custom_all_reduce_utils.py:234] reading GPU P2P access cache from /user/al4263/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 09-22 21:14:14 custom_all_reduce_utils.py:234] reading GPU P2P access cache from /user/al4263/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1725760)\u001b[0;0m INFO 09-22 21:14:14 custom_all_reduce_utils.py:234] reading GPU P2P access cache from /user/al4263/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 09-22 21:14:15 shm_broadcast.py:235] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f31830eb770>, local_subscribe_port=48723, remote_subscribe_port=None)\n",
      "INFO 09-22 21:14:15 model_runner.py:720] Starting to load model meta-llama/Meta-Llama-3.1-70B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1725755)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1725754)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1725758)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1725756)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1725757)\u001b[0;0m INFO 09-22 21:14:15 model_runner.py:720] Starting to load model meta-llama/Meta-Llama-3.1-70B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1725759)\u001b[0;0m INFO 09-22 21:14:15 model_runner.py:720] Starting to load model meta-llama/Meta-Llama-3.1-70B-Instruct...\n",
      "INFO 09-22 21:14:15 model_runner.py:720] Starting to load model meta-llama/Meta-Llama-3.1-70B-Instruct...\n",
      "INFO 09-22 21:14:15 model_runner.py:720] Starting to load model meta-llama/Meta-Llama-3.1-70B-Instruct...\n",
      "INFO 09-22 21:14:15 model_runner.py:720] Starting to load model meta-llama/Meta-Llama-3.1-70B-Instruct...\n",
      "INFO 09-22 21:14:15 model_runner.py:720] Starting to load model meta-llama/Meta-Llama-3.1-70B-Instruct...\n",
      "INFO 09-22 21:14:15 model_runner.py:720] Starting to load model meta-llama/Meta-Llama-3.1-70B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1725760)\u001b[0;0m INFO 09-22 21:14:15 weight_utils.py:225] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1725759)\u001b[0;0m INFO 09-22 21:14:15 weight_utils.py:225] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1725756)\u001b[0;0m INFO 09-22 21:14:15 weight_utils.py:225] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1725758)\u001b[0;0m INFO 09-22 21:14:15 weight_utils.py:225] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1725754)\u001b[0;0m INFO 09-22 21:14:15 weight_utils.py:225] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1725760)\u001b[0;0m INFO 09-22 21:14:15 weight_utils.py:225] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1725757)\u001b[0;0m INFO 09-22 21:14:15 weight_utils.py:225] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/30 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:   3% Completed | 1/30 [00:00<00:08,  3.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=1725755)\u001b[0;0m INFO 09-22 21:14:16 weight_utils.py:225] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   7% Completed | 2/30 [00:00<00:09,  2.83it/s]\n",
      "Loading safetensors checkpoint shards:  10% Completed | 3/30 [00:01<00:10,  2.61it/s]\n",
      "Loading safetensors checkpoint shards:  13% Completed | 4/30 [00:01<00:10,  2.52it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 5/30 [00:01<00:10,  2.49it/s]\n",
      "Loading safetensors checkpoint shards:  20% Completed | 6/30 [00:02<00:09,  2.46it/s]\n",
      "Loading safetensors checkpoint shards:  23% Completed | 7/30 [00:02<00:09,  2.46it/s]\n",
      "Loading safetensors checkpoint shards:  27% Completed | 8/30 [00:03<00:08,  2.49it/s]\n",
      "Loading safetensors checkpoint shards:  30% Completed | 9/30 [00:03<00:08,  2.53it/s]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 10/30 [00:03<00:07,  2.53it/s]\n",
      "Loading safetensors checkpoint shards:  37% Completed | 11/30 [00:04<00:07,  2.48it/s]\n",
      "Loading safetensors checkpoint shards:  40% Completed | 12/30 [00:04<00:06,  2.60it/s]\n",
      "Loading safetensors checkpoint shards:  43% Completed | 13/30 [00:05<00:06,  2.60it/s]\n",
      "Loading safetensors checkpoint shards:  47% Completed | 14/30 [00:05<00:06,  2.59it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 15/30 [00:05<00:05,  2.57it/s]\n",
      "Loading safetensors checkpoint shards:  53% Completed | 16/30 [00:06<00:05,  2.55it/s]\n",
      "Loading safetensors checkpoint shards:  57% Completed | 17/30 [00:06<00:04,  2.63it/s]\n",
      "Loading safetensors checkpoint shards:  60% Completed | 18/30 [00:07<00:04,  2.52it/s]\n",
      "Loading safetensors checkpoint shards:  63% Completed | 19/30 [00:07<00:04,  2.51it/s]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 20/30 [00:07<00:03,  2.80it/s]\n",
      "Loading safetensors checkpoint shards:  70% Completed | 21/30 [00:08<00:03,  2.77it/s]\n",
      "Loading safetensors checkpoint shards:  73% Completed | 22/30 [00:08<00:02,  2.86it/s]\n",
      "Loading safetensors checkpoint shards:  77% Completed | 23/30 [00:08<00:01,  3.52it/s]\n",
      "Loading safetensors checkpoint shards:  80% Completed | 24/30 [00:08<00:01,  3.31it/s]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 25/30 [00:09<00:01,  3.22it/s]\n",
      "Loading safetensors checkpoint shards:  87% Completed | 26/30 [00:09<00:01,  2.99it/s]\n",
      "Loading safetensors checkpoint shards:  90% Completed | 27/30 [00:10<00:01,  2.81it/s]\n",
      "Loading safetensors checkpoint shards:  93% Completed | 28/30 [00:10<00:00,  2.86it/s]\n",
      "Loading safetensors checkpoint shards:  97% Completed | 29/30 [00:10<00:00,  2.77it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 30/30 [00:11<00:00,  2.72it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 30/30 [00:11<00:00,  2.70it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-22 21:14:27 model_runner.py:732] Loading model weights took 16.4605 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1725758)\u001b[0;0m INFO 09-22 21:14:27 model_runner.py:732] Loading model weights took 16.4605 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1725755)\u001b[0;0m INFO 09-22 21:14:27 model_runner.py:732] Loading model weights took 16.4605 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1725757)\u001b[0;0m INFO 09-22 21:14:27 model_runner.py:732] Loading model weights took 16.4605 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1725756)\u001b[0;0m INFO 09-22 21:14:28 model_runner.py:732] Loading model weights took 16.4605 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1725754)\u001b[0;0m INFO 09-22 21:14:28 model_runner.py:732] Loading model weights took 16.4605 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1725759)\u001b[0;0m INFO 09-22 21:14:29 model_runner.py:732] Loading model weights took 16.4605 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1725760)\u001b[0;0m INFO 09-22 21:14:30 model_runner.py:732] Loading model weights took 16.4605 GB\n",
      "INFO 09-22 21:14:33 distributed_gpu_executor.py:56] # GPU blocks: 83177, # CPU blocks: 6553\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1725759)\u001b[0;0m INFO 09-22 21:14:37 model_runner.py:1024] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1725759)\u001b[0;0m INFO 09-22 21:14:37 model_runner.py:1028] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1725760)\u001b[0;0m INFO 09-22 21:14:37 model_runner.py:1024] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1725760)\u001b[0;0m INFO 09-22 21:14:37 model_runner.py:1028] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1725754)\u001b[0;0m INFO 09-22 21:14:37 model_runner.py:1024] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1725754)\u001b[0;0m INFO 09-22 21:14:37 model_runner.py:1028] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 09-22 21:14:37 model_runner.py:1024] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 09-22 21:14:37 model_runner.py:1028] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1725758)\u001b[0;0m INFO 09-22 21:14:37 model_runner.py:1024] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1725755)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1725756)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1725757)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1725758)\u001b[0;0m INFO 09-22 21:14:37 model_runner.py:1024] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 09-22 21:14:37 model_runner.py:1028] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 09-22 21:14:37 model_runner.py:1024] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 09-22 21:14:37 model_runner.py:1024] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1725755)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1725757)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1725756)\u001b[0;0m INFO 09-22 21:14:37 model_runner.py:1028] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 09-22 21:14:37 model_runner.py:1028] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 09-22 21:14:37 model_runner.py:1028] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 09-22 21:14:51 custom_all_reduce.py:219] Registering 5635 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1725754)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1725758)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1725757)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1725760)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1725756)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1725755)\u001b[0;0m INFO 09-22 21:14:51 custom_all_reduce.py:219] Registering 5635 cuda graph addresses\n",
      "INFO 09-22 21:14:51 custom_all_reduce.py:219] Registering 5635 cuda graph addresses\n",
      "INFO 09-22 21:14:51 custom_all_reduce.py:219] Registering 5635 cuda graph addresses\n",
      "INFO 09-22 21:14:51 custom_all_reduce.py:219] Registering 5635 cuda graph addresses\n",
      "INFO 09-22 21:14:51 custom_all_reduce.py:219] Registering 5635 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1725759)\u001b[0;0m INFO 09-22 21:14:51 custom_all_reduce.py:219] Registering 5635 cuda graph addresses\n",
      "INFO 09-22 21:14:51 custom_all_reduce.py:219] Registering 5635 cuda graph addresses\n",
      "INFO 09-22 21:14:51 model_runner.py:1225] Graph capturing finished in 13 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1725754)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1725755)\u001b[0;0m INFO 09-22 21:14:51 model_runner.py:1225] Graph capturing finished in 13 secs.\n",
      "INFO 09-22 21:14:51 model_runner.py:1225] Graph capturing finished in 13 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1725756)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1725760)\u001b[0;0m INFO 09-22 21:14:51 model_runner.py:1225] Graph capturing finished in 13 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1725759)\u001b[0;0m INFO 09-22 21:14:51 model_runner.py:1225] Graph capturing finished in 14 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1725757)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1725758)\u001b[0;0m INFO 09-22 21:14:51 model_runner.py:1225] Graph capturing finished in 14 secs.\n",
      "INFO 09-22 21:14:51 model_runner.py:1225] Graph capturing finished in 13 secs.\n",
      "INFO 09-22 21:14:51 model_runner.py:1225] Graph capturing finished in 13 secs.\n",
      "Model meta-llama/Meta-Llama-3.1-70B-Instruct loaded successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=1725758)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1725754)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1725756)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1725755)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1725757)\u001b[0;0m WARNING 09-22 21:51:58 shm_broadcast.py:386] No available block found in 60 second. \n",
      "\u001b[1;36m(VllmWorkerProcess pid=1725759)\u001b[0;0m WARNING 09-22 21:51:58 shm_broadcast.py:386] No available block found in 60 second. \n",
      "\u001b[1;36m(VllmWorkerProcess pid=1725760)\u001b[0;0m WARNING 09-22 21:51:58 shm_broadcast.py:386] No available block found in 60 second. \n",
      "WARNING 09-22 21:51:58 shm_broadcast.py:386] No available block found in 60 second. \n",
      "WARNING 09-22 21:51:58 shm_broadcast.py:386] No available block found in 60 second. \n",
      "WARNING 09-22 21:51:58 shm_broadcast.py:386] No available block found in 60 second. \n",
      "WARNING 09-22 21:51:58 shm_broadcast.py:386] No available block found in 60 second. \n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# Set up cache directories\n",
    "HF_HOME_DIR = \"/shared/share_mala/Leon/huggingface/cache\"\n",
    "TRANSFORMERS_CACHE_DIR = \"/shared/share_mala/Leon/huggingface/cache\"\n",
    "os.makedirs(HF_HOME_DIR, exist_ok=True)\n",
    "os.makedirs(TRANSFORMERS_CACHE_DIR, exist_ok=True)\n",
    "\n",
    "os.environ['HF_HOME'] = HF_HOME_DIR\n",
    "os.environ['TRANSFORMERS_CACHE'] = TRANSFORMERS_CACHE_DIR\n",
    "\n",
    "# Function to load the LLM\n",
    "def load_llm(model_name):\n",
    "    try:\n",
    "        llm = LLM(model=model_name, tensor_parallel_size=8, gpu_memory_utilization=0.9, download_dir=HF_HOME_DIR)\n",
    "        print(f\"Model {model_name} loaded successfully.\")\n",
    "        return llm\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing LLM: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to generate text\n",
    "def generate_text(llm, prompt, max_tokens=100, temperature=0.7, top_p=0.95):\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "    outputs = llm.generate([prompt], sampling_params)\n",
    "    return outputs[0].outputs[0].text\n",
    "\n",
    "# Load the model\n",
    "model_name = \"meta-llama/Meta-Llama-3.1-70B-Instruct\"  # You can change this to the specific model you want to use\n",
    "llm = load_llm(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer for meta-llama/Meta-Llama-3.1-70B-Instruct loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "def load_tokenizer(model_name):\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "        print(f\"Tokenizer for {model_name} loaded successfully.\")\n",
    "        return tokenizer\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading tokenizer: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"meta-llama/Meta-Llama-3.1-70B-Instruct\"  # Adjust this to your specific LLaMA model\n",
    "tokenizer = load_tokenizer(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an AI assistant specialized in demographic data verification and detailed persona generation for economic and political policy response simulation. Your task is to generate a specific, realistic persona based on the provided demographic information.\n",
      "\n",
      "###Instructions:###\n",
      "1. Create a detailed, concrete persona that is fully consistent with ALL features in the given metadata.\n",
      "2. For any ranges or categories provided in the metadata, select and specify exact values or details within those ranges/categories.\n",
      "3. Elaborate on all metadata points, providing specific details that flesh out the persona while remaining true to the given information.\n",
      "4. For any null or missing values in the metadata, make a reasonable assumption based on other provided information, and clearly state this assumption.\n",
      "5. Focus on characteristics that could influence economic and political views, including but not limited to:\n",
      "   * Precise economic situation (exact income, specific job title and sector, details of homeownership or renting situation)\n",
      "   * Detailed educational background and its impact on career trajectory\n",
      "   * Specific family structure and responsibilities\n",
      "   * Exact geographic location (city/town name, neighborhood type) and its economic implications\n",
      "   * Concrete economic concerns or priorities based on the persona's situation\n",
      "6. Avoid creating a narrative, but provide enough specific details to make the persona feel real and three-dimensional.\n",
      "7. Maintain diversity by acknowledging various experiences within the demographic group, but commit to specific details for this individual persona.\n",
      "\n",
      "###Response Format:###\n",
      "Persona: [A detailed, objective description of a specific individual, focusing on factors relevant to economic and political views. Ensure all details are consistent with and elaborate upon the provided metadata.]\n",
      "\n",
      "###Additional Information:###\n",
      "Remember, this persona will be used for simulation purposes, so it needs to be realistic, specific, and fully fleshed out while adhering strictly to the given demographic information. Ensure the response strictly follows the format: Persona: [description]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "system_message = json.load(open(\"/user/al4263/Simulate/Persona/prompts/persona_generation.json\", \"r\"))[\"system_message\"]\n",
    "\n",
    "print(system_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "def generate_text(llm, prompt, max_tokens=1000, temperature=0.7, top_p=0.95):\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "    outputs = llm.generate([prompt], sampling_params)\n",
    "\n",
    "    return outputs[0].outputs[0].text\n",
    "\n",
    "def extract_metadata_and_persona(output):\n",
    "    # Find the start of the metadata and persona information\n",
    "    metadata_start = output.find(\"Revised Metadata:\")\n",
    "    persona_start = output.find(\"Persona:\")\n",
    "    \n",
    "    if metadata_start == -1 or persona_start == -1:\n",
    "        return \"Metadata or Persona information not found.\", \"Information not found.\"\n",
    "    \n",
    "    # Extract the metadata text\n",
    "    metadata_text = output[metadata_start + 16:persona_start].strip()\n",
    "    \n",
    "    # Extract the persona text\n",
    "    persona_text = output[persona_start + 8:].strip()\n",
    "    \n",
    "    return metadata_text, persona_text\n",
    "\n",
    "def extract_persona(output):\n",
    "    return output.split(\"Persona: \")[1].strip()\n",
    "\n",
    "def save_metadata_and_persona(state, index, metadata, persona):\n",
    "    base_dir = f\"/user/al4263/Simulate/Persona/data/persona_llama_generation/{state.abbr}\"\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "    \n",
    "    file_name = f\"{state.abbr}_persona_{index}.json\"\n",
    "    file_path = os.path.join(base_dir, file_name)\n",
    "    \n",
    "    # Check if metadata is a string, if so, try to parse it as JSON\n",
    "    if isinstance(metadata, str):\n",
    "        # Remove the ': \\n' prefix if it exists\n",
    "        metadata = metadata.lstrip(': \\n')\n",
    "        try:\n",
    "            metadata = json.loads(metadata)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Error parsing metadata for {file_name}. Using empty dictionary.\")\n",
    "            metadata = {}\n",
    "    \n",
    "    data = metadata.copy()\n",
    "    data['PERSONA'] = persona\n",
    "    \n",
    "    with open(file_path, \"w\") as file:\n",
    "        json.dump(data, file, indent=2)\n",
    "\n",
    "def save_persona(state, index, persona):\n",
    "    base_dir = f\"/user/al4263/Simulate/Persona/data/persona_llama_generation/{state.abbr}\"\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "    \n",
    "    file_name = f\"{state.abbr}_persona_{index}.json\"\n",
    "    file_path = os.path.join(base_dir, file_name)\n",
    "\n",
    "    data = {\n",
    "        \"PERSONA\": persona\n",
    "    }\n",
    "    \n",
    "    with open(file_path, \"w\") as file:\n",
    "        json.dump(data, file, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Wyoming personas:   0%|          | 0/1000 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 2/2 [00:08<00:00,  4.33s/it, est. speed input: 153.96 toks/s, output: 90.95 toks/s]\n",
      "Processing Wyoming personas:   0%|          | 1/1000 [00:08<2:24:28,  8.68s/it]\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.80s/it, est. speed input: 113.94 toks/s, output: 51.71 toks/s]\n",
      "Processing Wyoming personas:   0%|          | 2/1000 [00:14<1:56:22,  7.00s/it]\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.65s/it, est. speed input: 86.33 toks/s, output: 51.80 toks/s]\n",
      "Processing Wyoming personas:   0%|          | 3/1000 [00:22<2:01:19,  7.30s/it]\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:09<00:00,  9.95s/it, est. speed input: 66.98 toks/s, output: 51.90 toks/s]\n",
      "Processing Wyoming personas:   0%|          | 4/1000 [00:32<2:18:35,  8.35s/it]\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.64s/it, est. speed input: 86.76 toks/s, output: 52.08 toks/s]\n",
      "Processing Wyoming personas:   0%|          | 5/1000 [00:39<2:14:18,  8.10s/it]\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.44s/it, est. speed input: 103.27 toks/s, output: 52.02 toks/s]\n",
      "Processing Wyoming personas:   1%|          | 6/1000 [00:46<2:04:53,  7.54s/it]\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.76s/it, est. speed input: 75.70 toks/s, output: 51.83 toks/s]\n",
      "Processing Wyoming personas:   1%|          | 7/1000 [00:54<2:11:25,  7.94s/it]\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:12<00:00, 12.08s/it, est. speed input: 54.81 toks/s, output: 52.08 toks/s]\n",
      "Processing Wyoming personas:   1%|          | 8/1000 [01:07<2:33:08,  9.26s/it]\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.03s/it, est. speed input: 82.50 toks/s, output: 51.59 toks/s]\n",
      "Processing Wyoming personas:   1%|          | 9/1000 [01:15<2:26:39,  8.88s/it]\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.28s/it, est. speed input: 125.21 toks/s, output: 51.53 toks/s]\n",
      "Processing Wyoming personas:   1%|          | 10/1000 [01:20<2:08:14,  7.77s/it]\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.48s/it, est. speed input: 88.22 toks/s, output: 51.86 toks/s]\n",
      "Processing Wyoming personas:   1%|          | 11/1000 [01:27<2:06:42,  7.69s/it]\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.34s/it, est. speed input: 79.63 toks/s, output: 52.05 toks/s]\n",
      "Processing Wyoming personas:   1%|          | 12/1000 [01:36<2:09:54,  7.89s/it]\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.99s/it, est. speed input: 83.04 toks/s, output: 51.85 toks/s]\n",
      "Processing Wyoming personas:   1%|▏         | 13/1000 [01:44<2:10:18,  7.92s/it]\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.43s/it, est. speed input: 89.23 toks/s, output: 52.08 toks/s]\n",
      "Processing Wyoming personas:   1%|▏         | 14/1000 [01:51<2:07:48,  7.78s/it]\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.89s/it, est. speed input: 84.51 toks/s, output: 52.20 toks/s]\n",
      "Processing Wyoming personas:   2%|▏         | 15/1000 [01:59<2:08:17,  7.82s/it]\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.24s/it, est. speed input: 81.11 toks/s, output: 52.09 toks/s]\n",
      "Processing Wyoming personas:   2%|▏         | 16/1000 [02:07<2:10:18,  7.95s/it]\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.75s/it, est. speed input: 76.23 toks/s, output: 51.77 toks/s]\n",
      "Processing Wyoming personas:   2%|▏         | 17/1000 [02:16<2:14:12,  8.19s/it]\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.84s/it, est. speed input: 84.60 toks/s, output: 51.93 toks/s]\n",
      "Processing Wyoming personas:   2%|▏         | 18/1000 [02:24<2:12:23,  8.09s/it]\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.25s/it, est. speed input: 91.01 toks/s, output: 51.99 toks/s]\n",
      "Processing Wyoming personas:   2%|▏         | 19/1000 [02:31<2:08:11,  7.84s/it]\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.96s/it, est. speed input: 83.35 toks/s, output: 52.05 toks/s]\n",
      "Processing Wyoming personas:   2%|▏         | 20/1000 [02:39<2:08:40,  7.88s/it]\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:09<00:00,  9.80s/it, est. speed input: 67.15 toks/s, output: 51.44 toks/s]\n",
      "Processing Wyoming personas:   2%|▏         | 21/1000 [02:49<2:18:00,  8.46s/it]\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:09<00:00,  9.62s/it, est. speed input: 68.92 toks/s, output: 51.87 toks/s]\n",
      "Processing Wyoming personas:   2%|▏         | 22/1000 [02:59<2:23:37,  8.81s/it]\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.16s/it, est. speed input: 81.24 toks/s, output: 52.20 toks/s]\n",
      "Processing Wyoming personas:   2%|▏         | 23/1000 [03:07<2:20:21,  8.62s/it]\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.43s/it, est. speed input: 89.16 toks/s, output: 51.59 toks/s]\n",
      "Processing Wyoming personas:   2%|▏         | 24/1000 [03:14<2:14:26,  8.26s/it]\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.47s/it, est. speed input: 78.17 toks/s, output: 52.07 toks/s]\n",
      "Processing Wyoming personas:   2%|▎         | 25/1000 [03:23<2:15:21,  8.33s/it]\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.85s/it, est. speed input: 84.82 toks/s, output: 52.09 toks/s]\n",
      "Processing Wyoming personas:   3%|▎         | 26/1000 [03:31<2:12:57,  8.19s/it]\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.18s/it, est. speed input: 107.35 toks/s, output: 52.30 toks/s]\n",
      "Processing Wyoming personas:   3%|▎         | 27/1000 [03:37<2:03:05,  7.59s/it]\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.52s/it, est. speed input: 101.54 toks/s, output: 51.84 toks/s]\n",
      "Processing Wyoming personas:   3%|▎         | 28/1000 [03:43<1:57:49,  7.27s/it]\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.53s/it, est. speed input: 77.13 toks/s, output: 52.05 toks/s]\n",
      "Processing Wyoming personas:   3%|▎         | 29/1000 [03:52<2:03:52,  7.65s/it]\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.80s/it, est. speed input: 61.80 toks/s, output: 51.98 toks/s]\n",
      "Processing Wyoming personas:   3%|▎         | 30/1000 [04:03<2:19:01,  8.60s/it]\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.90s/it, est. speed input: 61.21 toks/s, output: 52.31 toks/s]\n",
      "Processing Wyoming personas:   3%|▎         | 31/1000 [04:14<2:30:05,  9.29s/it]\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.32s/it, est. speed input: 79.43 toks/s, output: 52.15 toks/s]\n",
      "Processing Wyoming personas:   3%|▎         | 32/1000 [04:22<2:25:17,  9.01s/it]\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.76s/it, est. speed input: 77.38 toks/s, output: 52.27 toks/s]\n",
      "Processing Wyoming personas:   3%|▎         | 33/1000 [04:31<2:24:01,  8.94s/it]\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.26s/it, est. speed input: 80.76 toks/s, output: 51.82 toks/s]\n",
      "Processing Wyoming personas:   3%|▎         | 34/1000 [04:39<2:20:40,  8.74s/it]\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.95s/it, est. speed input: 83.59 toks/s, output: 51.99 toks/s]\n",
      "Processing Wyoming personas:   4%|▎         | 35/1000 [04:47<2:16:45,  8.50s/it]\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.07s/it, est. speed input: 83.02 toks/s, output: 51.79 toks/s]\n",
      "Processing Wyoming personas:   4%|▎         | 36/1000 [04:55<2:14:35,  8.38s/it]\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.49s/it, est. speed input: 88.21 toks/s, output: 52.18 toks/s]\n",
      "Processing Wyoming personas:   4%|▎         | 37/1000 [05:03<2:10:14,  8.12s/it]\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.60s/it, est. speed input: 76.96 toks/s, output: 51.73 toks/s]\n",
      "Processing Wyoming personas:   4%|▍         | 38/1000 [05:11<2:12:32,  8.27s/it]\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.20s/it, est. speed input: 106.64 toks/s, output: 52.11 toks/s]\n",
      "Processing Wyoming personas:   4%|▍         | 39/1000 [05:17<2:02:31,  7.65s/it]\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.43s/it, est. speed input: 89.36 toks/s, output: 51.54 toks/s]\n",
      "Processing Wyoming personas:   4%|▍         | 40/1000 [05:25<2:01:24,  7.59s/it]\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:09<00:00,  9.57s/it, est. speed input: 69.26 toks/s, output: 51.81 toks/s]\n",
      "Processing Wyoming personas:   4%|▍         | 41/1000 [05:34<2:10:51,  8.19s/it]\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.23s/it, est. speed input: 106.94 toks/s, output: 52.35 toks/s]\n",
      "Processing Wyoming personas:   4%|▍         | 42/1000 [05:41<2:01:23,  7.60s/it]\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.81s/it, est. speed input: 97.57 toks/s, output: 52.90 toks/s]\n",
      "Processing Wyoming personas:   4%|▍         | 43/1000 [05:47<1:57:30,  7.37s/it]\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.22s/it, est. speed input: 107.19 toks/s, output: 51.75 toks/s]\n",
      "Processing Wyoming personas:   4%|▍         | 44/1000 [05:54<1:51:58,  7.03s/it]\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.33s/it, est. speed input: 90.78 toks/s, output: 52.56 toks/s]\n",
      "Processing Wyoming personas:   4%|▍         | 45/1000 [06:01<1:53:19,  7.12s/it]\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.66s/it, est. speed input: 87.40 toks/s, output: 51.74 toks/s]\n",
      "Processing Wyoming personas:   5%|▍         | 46/1000 [06:09<1:55:48,  7.28s/it]\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.17s/it, est. speed input: 81.17 toks/s, output: 50.44 toks/s]\n",
      "Processing Wyoming personas:   5%|▍         | 47/1000 [06:17<1:59:57,  7.55s/it]\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.46s/it, est. speed input: 78.75 toks/s, output: 51.91 toks/s]\n",
      "Processing Wyoming personas:   5%|▍         | 48/1000 [06:25<2:04:11,  7.83s/it]\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:09<00:00,  9.02s/it, est. speed input: 73.59 toks/s, output: 51.65 toks/s]\n",
      "Processing Wyoming personas:   5%|▍         | 49/1000 [06:34<2:09:48,  8.19s/it]\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.84s/it, est. speed input: 96.55 toks/s, output: 52.37 toks/s]\n",
      "Processing Wyoming personas:   5%|▌         | 50/1000 [06:41<2:03:17,  7.79s/it]\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:09<00:00,  9.32s/it, est. speed input: 70.75 toks/s, output: 51.86 toks/s]\n",
      "Processing Wyoming personas:   5%|▌         | 51/1000 [06:51<2:10:27,  8.25s/it]\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.61s/it, est. speed input: 77.65 toks/s, output: 53.01 toks/s]\n",
      "Processing Wyoming personas:   5%|▌         | 52/1000 [06:59<2:12:03,  8.36s/it]\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.66s/it, est. speed input: 77.92 toks/s, output: 51.83 toks/s]\n",
      "Processing Wyoming personas:   5%|▌         | 53/1000 [07:08<2:13:25,  8.45s/it]\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.65s/it, est. speed input: 86.53 toks/s, output: 52.15 toks/s]\n",
      "Processing Wyoming personas:   5%|▌         | 54/1000 [07:15<2:09:33,  8.22s/it]\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.49s/it, est. speed input: 78.20 toks/s, output: 53.23 toks/s]\n",
      "Processing Wyoming personas:   6%|▌         | 55/1000 [07:24<2:10:45,  8.30s/it]\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.86s/it, est. speed input: 98.46 toks/s, output: 52.95 toks/s]\n",
      "Processing Wyoming personas:   6%|▌         | 56/1000 [07:31<2:03:50,  7.87s/it]\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.85s/it, est. speed input: 85.35 toks/s, output: 52.36 toks/s]\n",
      "Processing Wyoming personas:   6%|▌         | 57/1000 [07:39<2:03:40,  7.87s/it]\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:09<00:00,  9.12s/it, est. speed input: 73.03 toks/s, output: 52.42 toks/s]\n",
      "Processing Wyoming personas:   6%|▌         | 58/1000 [07:48<2:09:29,  8.25s/it]\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.55s/it, est. speed input: 87.44 toks/s, output: 52.20 toks/s]\n",
      "Processing Wyoming personas:   6%|▌         | 59/1000 [07:55<2:06:06,  8.04s/it]\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:09<00:00,  9.54s/it, est. speed input: 69.97 toks/s, output: 52.34 toks/s]\n",
      "Processing Wyoming personas:   6%|▌         | 60/1000 [08:05<2:13:03,  8.49s/it]\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.63s/it, est. speed input: 77.35 toks/s, output: 51.72 toks/s]\n",
      "Processing Wyoming personas:   6%|▌         | 61/1000 [08:14<2:13:35,  8.54s/it]\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.80s/it, est. speed input: 137.10 toks/s, output: 52.09 toks/s]\n",
      "Processing Wyoming personas:   6%|▌         | 62/1000 [08:18<1:55:58,  7.42s/it]\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.54s/it, est. speed input: 77.40 toks/s, output: 51.99 toks/s]\n",
      "Processing Wyoming personas:   6%|▋         | 63/1000 [08:27<2:01:09,  7.76s/it]\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.29s/it, est. speed input: 91.67 toks/s, output: 52.01 toks/s]\n",
      "Processing Wyoming personas:   6%|▋         | 64/1000 [08:34<1:58:52,  7.62s/it]\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.97s/it, est. speed input: 83.02 toks/s, output: 51.87 toks/s]\n",
      "Processing Wyoming personas:   6%|▋         | 65/1000 [08:42<2:00:24,  7.73s/it]\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:09<00:00,  9.97s/it, est. speed input: 66.44 toks/s, output: 52.39 toks/s]\n",
      "Processing Wyoming personas:   7%|▋         | 66/1000 [08:52<2:10:47,  8.40s/it]\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.21s/it, est. speed input: 92.43 toks/s, output: 51.63 toks/s]\n",
      "Processing Wyoming personas:   7%|▋         | 67/1000 [08:59<2:05:06,  8.05s/it]\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.63s/it, est. speed input: 76.00 toks/s, output: 52.13 toks/s]\n",
      "Processing Wyoming personas:   7%|▋         | 68/1000 [09:08<2:07:46,  8.23s/it]\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.35s/it, est. speed input: 79.55 toks/s, output: 51.87 toks/s]\n",
      "Processing Wyoming personas:   7%|▋         | 69/1000 [09:16<2:08:14,  8.27s/it]\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.23s/it, est. speed input: 92.54 toks/s, output: 52.70 toks/s]\n",
      "Processing Wyoming personas:   7%|▋         | 70/1000 [09:24<2:03:20,  7.96s/it]\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.18s/it, est. speed input: 92.09 toks/s, output: 52.24 toks/s]\n",
      "Processing Wyoming personas:   7%|▋         | 71/1000 [09:31<1:59:38,  7.73s/it]\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.18s/it, est. speed input: 92.50 toks/s, output: 52.38 toks/s]\n",
      "Processing Wyoming personas:   7%|▋         | 72/1000 [09:38<1:57:00,  7.57s/it]\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.43s/it, est. speed input: 88.94 toks/s, output: 53.01 toks/s]\n",
      "Processing Wyoming personas:   7%|▋         | 73/1000 [09:45<1:56:19,  7.53s/it]\n",
      "\u001b[A"
     ]
    }
   ],
   "source": [
    "import us\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_state_personas(state):\n",
    "    base_dir = f\"/user/al4263/Simulate/Persona/data/persona_meta/{state.abbr}\"\n",
    "    files = [f for f in os.listdir(base_dir) if f.endswith('.json')]\n",
    "    \n",
    "    for i in tqdm(range(1000), desc=f\"Processing {state.name} personas\"):\n",
    "        file_name = f\"{state.abbr}_persona_{i}.json\"\n",
    "        file_path = os.path.join(base_dir, file_name)\n",
    "        \n",
    "        with open(file_path, \"r\") as f:\n",
    "            metadata = json.load(f)\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": f\"Based on the following verified metadata, please generate a detailed persona description:\\n\\n{json.dumps(metadata, indent=2)}\"}\n",
    "        ]\n",
    "        prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "        output = generate_text(llm, prompt)\n",
    "        persona = extract_persona(output)    \n",
    "        \n",
    "        save_persona(state, i, persona)\n",
    "\n",
    "# Process personas for all states\n",
    "# for state in tqdm(us.states.STATES, desc=\"Processing states\"):\n",
    "#     process_state_personas(state)\n",
    "\n",
    "\n",
    "# print(\"Persona generation complete for all states.\")\n",
    "# do it for CA first\n",
    "process_state_personas(us.states.WY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Opinions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_topics(topic_data):\n",
    "    messages = []\n",
    "    \n",
    "    for subtopic, data in topic_data.items():\n",
    "        if subtopic.startswith('topic_'):\n",
    "            topic = data['topic']\n",
    "            question = data['question']\n",
    "            candidates = data['candidates']\n",
    "            \n",
    "            message = \"###QUESTION###\\n\\n\"\n",
    "            message += f\"Topic: {topic}\\n\\n\"\n",
    "            message += f\"Question: {question}\\n\\n\"\n",
    "            \n",
    "            message += \"Candidates' Stances:\\n\"\n",
    "            for choice, info in candidates.items():\n",
    "                name = info['name']\n",
    "                background = info['background_info']\n",
    "                stance = info['stance']\n",
    "                message += f\"- {choice}: {name} ({background})\\n  Stance: {stance}\\n\\n\"\n",
    "            \n",
    "        with open(\"/user/al4263/Simulate/Persona/prompts/opinion_simulation/user_instruction.json\", \"r\") as f:\n",
    "            instruction = json.load(f)\n",
    "            message += instruction[\"Instruction\"]\n",
    "            \n",
    "            messages.append(message)\n",
    "    \n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_persona(state, index):\n",
    "    file_path = f\"/user/al4263/Simulate/Persona/data/persona_llama_generation/{state.abbr}/{state.abbr}_persona_{index}.json\"\n",
    "    with open(file_path, \"r\") as f:\n",
    "        persona_data = json.load(f)\n",
    "    return persona_data\n",
    "\n",
    "def generate_opinion(llm, tokenizer, state, index, topic, system_message):\n",
    "    persona_system = system_message[\"system_message\"].format(PERSONA=load_persona(state, index)[\"PERSONA\"])\n",
    "    \n",
    "    user_message = topic\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": persona_system},\n",
    "        {\"role\": \"user\", \"content\": user_message}\n",
    "    ]\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "    output = generate_text(llm, prompt, max_tokens=500)\n",
    "    \n",
    "    return output\n",
    "\n",
    "def extract_reasoning_and_decision(output):\n",
    "    reasoning = output.split(\"Reasoning: \")[1].split(\"\\n\")[0].strip()\n",
    "    decision = output.split(\"Answer: \")[1].strip()\n",
    "    return reasoning, decision\n",
    "\n",
    "# load topic from prompts/election/economic_policy.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "def generate_opinions_for_state(state, num_personas, topic, system_message):\n",
    "    results = {\n",
    "        \"topic\": topic,\n",
    "        \"opinions\": [],\n",
    "        \"counts\": {\"A\": 0, \"B\": 0}\n",
    "    }\n",
    "    \n",
    "    for index in tqdm(range(num_personas), desc=f\"Generating opinions for {state.name}\"):\n",
    "        opinion = generate_opinion(llm, tokenizer, state, index, topic, system_message)\n",
    "        reasoning, decision = extract_reasoning_and_decision(opinion)\n",
    "        \n",
    "        persona_id = f\"{state.abbr}_{index:02d}\"\n",
    "        results[\"opinions\"].append({\n",
    "            \"id\": persona_id,\n",
    "            \"persona\": load_persona(state, index)[\"PERSONA\"],\n",
    "            \"reason\": reasoning,\n",
    "            \"decision\": decision\n",
    "        })\n",
    "        \n",
    "        # Count the decisions\n",
    "        decision_letter = decision.strip()[0].upper()\n",
    "        if decision_letter in ['A', 'B']:\n",
    "            results[\"counts\"][decision_letter] += 1\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating opinions for Wyoming:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an AI assistant tasked with generating realistic opinions based on a given persona and topic. Here's the persona information:\n",
      "\n",
      "###PERSONA INFORMATION###\n",
      "\n",
      "Ryan Thompson, a 38-year-old White male, is the primary householder of a non-traditional married-couple household in Cheyenne, Wyoming. Although listed as \"Never married\" due to not having a legally recognized marriage, Ryan and his long-term partner, Alex, have been in a committed relationship for over 8 years and consider themselves married in all aspects but the legal ceremony. They have chosen not to have children.\n",
      "\n",
      "Ryan holds a Bachelor's degree in Environmental Science from the University of Wyoming, which he completed in 2008. This educational background has been instrumental in his career as a Sustainability Consultant for a local firm, where he has worked for the past 10 years, earning a salary of $62,000 annually. His job involves assessing and implementing sustainable practices for businesses and individuals in Wyoming, a role that he is deeply passionate about.\n",
      "\n",
      "Born in Denver, Colorado, Ryan moved to Wyoming for college and decided to stay due to his love for the outdoors and the state's natural beauty. He speaks English fluently and has no known second language, although he has taken courses in basic Spanish for work-related purposes.\n",
      "\n",
      "Ryan and Alex own a modest, two-bedroom home in a suburban neighborhood of Cheyenne, which they purchased in 2015 for $230,000. Their mortgage payments and living expenses are manageable, thanks to Ryan's stable job and Alex's part-time income as a freelance graphic designer.\n",
      "\n",
      "Economically, Ryan's primary concerns are related to the impact of climate change on Wyoming's economy, especially the decline of traditional industries like coal mining, and the need for sustainable job creation. He is also keenly aware of the rising costs of living in Cheyenne, particularly housing prices, which he fears could push long-term residents out of the area. Politically, Ryan leans towards policies that support environmental conservation, renewable energy, and economic diversification, believing these are crucial for Wyoming's future prosperity.\n",
      "\n",
      "###YOUR TASK###\n",
      "\n",
      "You will be provided with a opinion based multiple choice question with some context. Your task is to put yourself into the mindset of the above persona, and then select the most fitting answer on the given topic, considering the persona's background, economic situation, and potential viewpoints. Your choice should be:\n",
      "1. Consistent with the persona's demographic information and likely experiences\n",
      "2. Nuanced and not stereotypical\n",
      "3. Focused on economic and political aspects of the topic\n",
      "\n",
      "###RESPONSE FORMAT###\n",
      "\n",
      "Reasoning: [A brief explanation of how the persona's background influenced this opinion]\n",
      "\n",
      "Answer: [The most fitting answer, either A, B, C, or D, based on the reasoning provided]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.82s/it, est. speed input: 326.63 toks/s, output: 49.76 toks/s]\n",
      "Generating opinions for Wyoming: 100%|██████████| 1/1 [00:02<00:00,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to /user/al4263/Simulate/Persona/data/opinions_WY.json\n",
      "Counts: Trump (A): 0, Harris (B): 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load topic from prompts/election/economic_policy.json\n",
    "with open(\"/user/al4263/Simulate/Persona/prompts/election/economic_policy.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "topic = generate_topics(data)[0]\n",
    "\n",
    "with open(\"/user/al4263/Simulate/Persona/prompts/opinion_simulation/system_instruction.json\", \"r\") as f:\n",
    "    system_message = json.load(f)\n",
    "\n",
    "# Set up the parameters\n",
    "state = us.states.WY\n",
    "num_personas = 1  # Adjust this number as needed\n",
    "\n",
    "# Generate opinions\n",
    "results = generate_opinions_for_state(state, num_personas, topic, system_message)\n",
    "\n",
    "# Save results to a JSON file\n",
    "output_dir = f\"/user/al4263/Simulate/Persona/data/simulation\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_file = f\"{output_dir}/opinions_{state.abbr}.json\"\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to {output_file}\")\n",
    "print(f\"Counts: Trump (A): {results['counts']['A']}, Harris (B): {results['counts']['B']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
