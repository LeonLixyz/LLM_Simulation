{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user/al4263/.conda/envs/llm_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-09-22 19:45:36,974\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-22 19:45:55 config.py:729] Defaulting to use mp for distributed inference\n",
      "WARNING 09-22 19:45:55 arg_utils.py:766] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 09-22 19:45:55 config.py:820] Chunked prefill is enabled with max_num_batched_tokens=512.\n",
      "INFO 09-22 19:45:55 llm_engine.py:174] Initializing an LLM engine (v0.5.4) with config: model='meta-llama/Meta-Llama-3.1-70B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-70B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir='/shared/share_mala/Leon/huggingface/cache', load_format=LoadFormat.AUTO, tensor_parallel_size=8, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-70B-Instruct, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "WARNING 09-22 19:45:56 multiproc_gpu_executor.py:59] Reducing Torch parallelism from 128 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-22 19:45:56 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1720211)\u001b[0;0m INFO 09-22 19:45:57 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1720209)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1720214)\u001b[0;0m INFO 09-22 19:45:57 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 09-22 19:45:57 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1720213)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1720210)\u001b[0;0m INFO 09-22 19:45:57 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1720208)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1720212)\u001b[0;0m INFO 09-22 19:45:57 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 09-22 19:45:57 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 09-22 19:45:57 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 09-22 19:46:02 utils.py:841] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1720209)\u001b[0;0m INFO 09-22 19:46:02 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1720214)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1720208)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1720211)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1720210)\u001b[0;0m INFO 09-22 19:46:02 utils.py:841] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1720212)\u001b[0;0m INFO 09-22 19:46:02 utils.py:841] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1720213)\u001b[0;0m INFO 09-22 19:46:02 utils.py:841] Found nccl from library libnccl.so.2\n",
      "INFO 09-22 19:46:02 utils.py:841] Found nccl from library libnccl.so.2\n",
      "INFO 09-22 19:46:02 utils.py:841] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1720209)\u001b[0;0m INFO 09-22 19:46:02 utils.py:841] Found nccl from library libnccl.so.2\n",
      "INFO 09-22 19:46:02 utils.py:841] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1720214)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1720208)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1720211)\u001b[0;0m INFO 09-22 19:46:02 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1720210)\u001b[0;0m INFO 09-22 19:46:02 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1720213)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1720212)\u001b[0;0m INFO 09-22 19:46:02 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 09-22 19:46:02 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 09-22 19:46:02 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 09-22 19:46:02 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 09-22 19:46:02 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 09-22 19:46:06 custom_all_reduce_utils.py:234] reading GPU P2P access cache from /user/al4263/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1720212)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1720210)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1720214)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1720213)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1720208)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1720209)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1720211)\u001b[0;0m INFO 09-22 19:46:06 custom_all_reduce_utils.py:234] reading GPU P2P access cache from /user/al4263/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 09-22 19:46:06 custom_all_reduce_utils.py:234] reading GPU P2P access cache from /user/al4263/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 09-22 19:46:06 custom_all_reduce_utils.py:234] reading GPU P2P access cache from /user/al4263/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 09-22 19:46:06 custom_all_reduce_utils.py:234] reading GPU P2P access cache from /user/al4263/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 09-22 19:46:06 custom_all_reduce_utils.py:234] reading GPU P2P access cache from /user/al4263/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 09-22 19:46:06 custom_all_reduce_utils.py:234] reading GPU P2P access cache from /user/al4263/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 09-22 19:46:06 custom_all_reduce_utils.py:234] reading GPU P2P access cache from /user/al4263/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 09-22 19:46:06 shm_broadcast.py:235] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f3696193a10>, local_subscribe_port=37991, remote_subscribe_port=None)\n",
      "INFO 09-22 19:46:06 model_runner.py:720] Starting to load model meta-llama/Meta-Llama-3.1-70B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1720209)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1720208)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1720214)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1720212)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1720210)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1720211)\u001b[0;0m INFO 09-22 19:46:06 model_runner.py:720] Starting to load model meta-llama/Meta-Llama-3.1-70B-Instruct...\n",
      "INFO 09-22 19:46:06 model_runner.py:720] Starting to load model meta-llama/Meta-Llama-3.1-70B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1720213)\u001b[0;0m INFO 09-22 19:46:06 model_runner.py:720] Starting to load model meta-llama/Meta-Llama-3.1-70B-Instruct...\n",
      "INFO 09-22 19:46:06 model_runner.py:720] Starting to load model meta-llama/Meta-Llama-3.1-70B-Instruct...\n",
      "INFO 09-22 19:46:06 model_runner.py:720] Starting to load model meta-llama/Meta-Llama-3.1-70B-Instruct...\n",
      "INFO 09-22 19:46:06 model_runner.py:720] Starting to load model meta-llama/Meta-Llama-3.1-70B-Instruct...\n",
      "INFO 09-22 19:46:06 model_runner.py:720] Starting to load model meta-llama/Meta-Llama-3.1-70B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1720210)\u001b[0;0m INFO 09-22 19:46:08 weight_utils.py:225] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1720209)\u001b[0;0m INFO 09-22 19:46:08 weight_utils.py:225] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1720212)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1720214)\u001b[0;0m INFO 09-22 19:46:08 weight_utils.py:225] Using model weights format ['*.safetensors']\n",
      "INFO 09-22 19:46:08 weight_utils.py:225] Using model weights format ['*.safetensors']\n",
      "INFO 09-22 19:46:08 weight_utils.py:225] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1720211)\u001b[0;0m INFO 09-22 19:46:08 weight_utils.py:225] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1720213)\u001b[0;0m INFO 09-22 19:46:08 weight_utils.py:225] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1720208)\u001b[0;0m INFO 09-22 19:46:08 weight_utils.py:225] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/30 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:   3% Completed | 1/30 [00:17<08:40, 17.96s/it]\n",
      "Loading safetensors checkpoint shards:   7% Completed | 2/30 [00:35<08:22, 17.94s/it]\n",
      "Loading safetensors checkpoint shards:  10% Completed | 3/30 [00:55<08:19, 18.49s/it]\n",
      "Loading safetensors checkpoint shards:  13% Completed | 4/30 [01:14<08:08, 18.78s/it]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 5/30 [01:34<08:04, 19.39s/it]\n",
      "Loading safetensors checkpoint shards:  20% Completed | 6/30 [01:54<07:51, 19.66s/it]\n",
      "Loading safetensors checkpoint shards:  23% Completed | 7/30 [02:14<07:29, 19.56s/it]\n",
      "Loading safetensors checkpoint shards:  27% Completed | 8/30 [02:35<07:22, 20.12s/it]\n",
      "Loading safetensors checkpoint shards:  30% Completed | 9/30 [02:56<07:05, 20.27s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 10/30 [03:15<06:40, 20.04s/it]\n",
      "Loading safetensors checkpoint shards:  37% Completed | 11/30 [03:35<06:16, 19.82s/it]\n",
      "Loading safetensors checkpoint shards:  40% Completed | 12/30 [03:55<06:01, 20.10s/it]\n",
      "Loading safetensors checkpoint shards:  43% Completed | 13/30 [04:15<05:40, 20.03s/it]\n",
      "Loading safetensors checkpoint shards:  47% Completed | 14/30 [04:36<05:26, 20.38s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 15/30 [04:57<05:06, 20.41s/it]\n",
      "Loading safetensors checkpoint shards:  53% Completed | 16/30 [05:17<04:46, 20.44s/it]\n",
      "Loading safetensors checkpoint shards:  57% Completed | 17/30 [05:38<04:25, 20.41s/it]\n",
      "Loading safetensors checkpoint shards:  60% Completed | 18/30 [05:57<04:01, 20.15s/it]\n",
      "Loading safetensors checkpoint shards:  63% Completed | 19/30 [06:17<03:41, 20.12s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 20/30 [06:39<03:25, 20.55s/it]\n",
      "Loading safetensors checkpoint shards:  70% Completed | 21/30 [06:58<03:02, 20.23s/it]\n",
      "Loading safetensors checkpoint shards:  73% Completed | 22/30 [07:19<02:42, 20.34s/it]\n",
      "Loading safetensors checkpoint shards:  77% Completed | 23/30 [07:28<01:58, 16.93s/it]\n",
      "Loading safetensors checkpoint shards:  80% Completed | 24/30 [07:50<01:50, 18.36s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 25/30 [08:11<01:37, 19.42s/it]\n",
      "Loading safetensors checkpoint shards:  87% Completed | 26/30 [08:33<01:20, 20.12s/it]\n",
      "Loading safetensors checkpoint shards:  90% Completed | 27/30 [08:52<00:59, 19.67s/it]\n",
      "Loading safetensors checkpoint shards:  93% Completed | 28/30 [09:13<00:40, 20.02s/it]\n",
      "Loading safetensors checkpoint shards:  97% Completed | 29/30 [09:31<00:19, 19.67s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 30/30 [09:53<00:00, 20.35s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 30/30 [09:53<00:00, 19.80s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-22 19:59:54 model_runner.py:732] Loading model weights took 16.4605 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1720208)\u001b[0;0m INFO 09-22 19:59:55 model_runner.py:732] Loading model weights took 16.4605 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1720214)\u001b[0;0m INFO 09-22 19:59:55 model_runner.py:732] Loading model weights took 16.4605 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1720209)\u001b[0;0m INFO 09-22 19:59:55 model_runner.py:732] Loading model weights took 16.4605 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1720211)\u001b[0;0m INFO 09-22 19:59:55 model_runner.py:732] Loading model weights took 16.4605 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1720210)\u001b[0;0m INFO 09-22 19:59:55 model_runner.py:732] Loading model weights took 16.4605 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1720212)\u001b[0;0m INFO 09-22 19:59:55 model_runner.py:732] Loading model weights took 16.4605 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1720213)\u001b[0;0m INFO 09-22 19:59:55 model_runner.py:732] Loading model weights took 16.4605 GB\n",
      "INFO 09-22 20:00:02 distributed_gpu_executor.py:56] # GPU blocks: 83177, # CPU blocks: 6553\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1720209)\u001b[0;0m INFO 09-22 20:00:07 model_runner.py:1024] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1720209)\u001b[0;0m INFO 09-22 20:00:07 model_runner.py:1028] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 09-22 20:00:07 model_runner.py:1024] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 09-22 20:00:07 model_runner.py:1028] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1720210)\u001b[0;0m INFO 09-22 20:00:07 model_runner.py:1024] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1720211)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1720210)\u001b[0;0m INFO 09-22 20:00:07 model_runner.py:1028] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 09-22 20:00:07 model_runner.py:1024] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1720211)\u001b[0;0m INFO 09-22 20:00:07 model_runner.py:1028] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1720213)\u001b[0;0m INFO 09-22 20:00:07 model_runner.py:1024] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1720213)\u001b[0;0m INFO 09-22 20:00:07 model_runner.py:1028] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1720214)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1720208)\u001b[0;0m INFO 09-22 20:00:07 model_runner.py:1024] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 09-22 20:00:07 model_runner.py:1024] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1720214)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1720208)\u001b[0;0m INFO 09-22 20:00:07 model_runner.py:1028] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 09-22 20:00:07 model_runner.py:1028] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1720212)\u001b[0;0m INFO 09-22 20:00:07 model_runner.py:1024] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1720212)\u001b[0;0m INFO 09-22 20:00:07 model_runner.py:1028] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 09-22 20:00:24 custom_all_reduce.py:219] Registering 5635 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1720214)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1720208)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1720212)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1720210)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1720213)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1720211)\u001b[0;0m INFO 09-22 20:00:24 custom_all_reduce.py:219] Registering 5635 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1720209)\u001b[0;0m INFO 09-22 20:00:24 custom_all_reduce.py:219] Registering 5635 cuda graph addresses\n",
      "INFO 09-22 20:00:24 custom_all_reduce.py:219] Registering 5635 cuda graph addresses\n",
      "INFO 09-22 20:00:24 custom_all_reduce.py:219] Registering 5635 cuda graph addresses\n",
      "INFO 09-22 20:00:24 custom_all_reduce.py:219] Registering 5635 cuda graph addresses\n",
      "INFO 09-22 20:00:24 custom_all_reduce.py:219] Registering 5635 cuda graph addresses\n",
      "INFO 09-22 20:00:24 custom_all_reduce.py:219] Registering 5635 cuda graph addresses\n",
      "INFO 09-22 20:00:24 model_runner.py:1225] Graph capturing finished in 17 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1720214)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1720208)\u001b[0;0m INFO 09-22 20:00:24 model_runner.py:1225] Graph capturing finished in 17 secs.\n",
      "INFO 09-22 20:00:24 model_runner.py:1225] Graph capturing finished in 17 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1720213)\u001b[0;0m INFO 09-22 20:00:24 model_runner.py:1225] Graph capturing finished in 17 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1720210)\u001b[0;0m INFO 09-22 20:00:24 model_runner.py:1225] Graph capturing finished in 17 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1720212)\u001b[0;0m INFO 09-22 20:00:24 model_runner.py:1225] Graph capturing finished in 17 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1720211)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1720209)\u001b[0;0m INFO 09-22 20:00:24 model_runner.py:1225] Graph capturing finished in 17 secs.\n",
      "INFO 09-22 20:00:24 model_runner.py:1225] Graph capturing finished in 17 secs.\n",
      "Model meta-llama/Meta-Llama-3.1-70B-Instruct loaded successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=1720214)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1720210)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1720211)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1720209)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1720212)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1720208)\u001b[0;0m WARNING 09-22 20:53:46 shm_broadcast.py:386] No available block found in 60 second. \n",
      "\u001b[1;36m(VllmWorkerProcess pid=1720213)\u001b[0;0m WARNING 09-22 20:53:46 shm_broadcast.py:386] No available block found in 60 second. \n",
      "WARNING 09-22 20:53:46 shm_broadcast.py:386] No available block found in 60 second. \n",
      "WARNING 09-22 20:53:46 shm_broadcast.py:386] No available block found in 60 second. \n",
      "WARNING 09-22 20:53:46 shm_broadcast.py:386] No available block found in 60 second. \n",
      "WARNING 09-22 20:53:46 shm_broadcast.py:386] No available block found in 60 second. \n",
      "WARNING 09-22 20:53:46 shm_broadcast.py:386] No available block found in 60 second. \n",
      "\u001b[1;36m(VllmWorkerProcess pid=1720214)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1720208)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1720213)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1720211)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1720212)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1720209)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1720210)\u001b[0;0m WARNING 09-22 20:57:22 shm_broadcast.py:386] No available block found in 60 second. \n",
      "WARNING 09-22 20:57:22 shm_broadcast.py:386] No available block found in 60 second. \n",
      "WARNING 09-22 20:57:22 shm_broadcast.py:386] No available block found in 60 second. \n",
      "WARNING 09-22 20:57:22 shm_broadcast.py:386] No available block found in 60 second. \n",
      "WARNING 09-22 20:57:22 shm_broadcast.py:386] No available block found in 60 second. \n",
      "WARNING 09-22 20:57:22 shm_broadcast.py:386] No available block found in 60 second. \n",
      "WARNING 09-22 20:57:22 shm_broadcast.py:386] No available block found in 60 second. \n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# Set up cache directories\n",
    "HF_HOME_DIR = \"/shared/share_mala/Leon/huggingface/cache\"\n",
    "TRANSFORMERS_CACHE_DIR = \"/shared/share_mala/Leon/huggingface/cache\"\n",
    "os.makedirs(HF_HOME_DIR, exist_ok=True)\n",
    "os.makedirs(TRANSFORMERS_CACHE_DIR, exist_ok=True)\n",
    "\n",
    "os.environ['HF_HOME'] = HF_HOME_DIR\n",
    "os.environ['TRANSFORMERS_CACHE'] = TRANSFORMERS_CACHE_DIR\n",
    "\n",
    "# Function to load the LLM\n",
    "def load_llm(model_name):\n",
    "    try:\n",
    "        llm = LLM(model=model_name, tensor_parallel_size=8, gpu_memory_utilization=0.9, download_dir=HF_HOME_DIR)\n",
    "        print(f\"Model {model_name} loaded successfully.\")\n",
    "        return llm\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing LLM: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to generate text\n",
    "def generate_text(llm, prompt, max_tokens=100, temperature=0.7, top_p=0.95):\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "    outputs = llm.generate([prompt], sampling_params)\n",
    "    return outputs[0].outputs[0].text\n",
    "\n",
    "# Load the model\n",
    "model_name = \"meta-llama/Meta-Llama-3.1-70B-Instruct\"  # You can change this to the specific model you want to use\n",
    "llm = load_llm(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer for meta-llama/Meta-Llama-3.1-70B-Instruct loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "def load_tokenizer(model_name):\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "        print(f\"Tokenizer for {model_name} loaded successfully.\")\n",
    "        return tokenizer\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading tokenizer: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"meta-llama/Meta-Llama-3.1-70B-Instruct\"  # Adjust this to your specific LLaMA model\n",
    "tokenizer = load_tokenizer(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "system_message = \"You are an AI assistant specialized in persona verification and concise character generation. Your tasks are:\\n\\n1. Verify Metadata: Quickly check the provided demographic information for logical consistency and real-world plausibility. If inconsistencies are found, change the metadata to be consistent.\\n\\n2. Generate Concise Persona: Based on the verified metadata, create a diverse while representative persona description. Add in all demographic information from the metadata as well as diverse attributes that are not in the metadata. The persona will be used for simulation, so write the persona in second person voice, i.e You are xxx.\\n\\nRespond in this format:\\nRevised Metadata: [Metadata]\\nPersona: [2-3 sentence description]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.56s/it, est. speed input: 83.81 toks/s, output: 51.43 toks/s]\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "def generate_text(llm, prompt, max_tokens=1000, temperature=0.7, top_p=0.95):\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "    outputs = llm.generate([prompt], sampling_params)\n",
    "\n",
    "    return outputs[0].outputs[0].text\n",
    "\n",
    "def extract_metadata_and_persona(output):\n",
    "    # Find the start of the metadata and persona information\n",
    "    metadata_start = output.find(\"Revised Metadata:\")\n",
    "    persona_start = output.find(\"Persona:\")\n",
    "    \n",
    "    if metadata_start == -1 or persona_start == -1:\n",
    "        return \"Metadata or Persona information not found.\", \"Information not found.\"\n",
    "    \n",
    "    # Extract the metadata text\n",
    "    metadata_text = output[metadata_start + 16:persona_start].strip()\n",
    "    \n",
    "    # Extract the persona text\n",
    "    persona_text = output[persona_start + 8:].strip()\n",
    "    \n",
    "    return metadata_text, persona_text\n",
    "\n",
    "def save_metadata_and_persona(state, index, metadata, persona):\n",
    "    base_dir = f\"/user/al4263/Simulate/Persona/data/persona_llama_generation/{state.abbr}\"\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "    \n",
    "    file_name = f\"{state.abbr}_persona_{index}.json\"\n",
    "    file_path = os.path.join(base_dir, file_name)\n",
    "    \n",
    "    # Check if metadata is a string, if so, try to parse it as JSON\n",
    "    if isinstance(metadata, str):\n",
    "        # Remove the ': \\n' prefix if it exists\n",
    "        metadata = metadata.lstrip(': \\n')\n",
    "        try:\n",
    "            metadata = json.loads(metadata)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Error parsing metadata for {file_name}. Using empty dictionary.\")\n",
    "            metadata = {}\n",
    "    \n",
    "    data = metadata.copy()\n",
    "    data['PERSONA'] = persona\n",
    "    \n",
    "    with open(file_path, \"w\") as file:\n",
    "        json.dump(data, file, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "': \\n{\\n  \"Age\": \"45 to 54 years\",\\n  \"Sex\": \"Male\",\\n  \"Race\": \"White and Some Other Race\",\\n  \"Ethnicity\": \"None\",\\n  \"HOUSEHOLD_RELATIONSHIP\": \"Homeless\",\\n  \"HOUSEHOLD_TYPE\": \"Single Male Without kids\",\\n  \"MARITAL_STATUS\": \"Divorced\",\\n  \"VETERAN_STATUS\": \"Non-Veteran\",\\n  \"LANGUAGE\": \"English only\",\\n  \"ENGLISH_PROFICIENCY\": \"Speak English well\",\\n  \"EDUCATION\": \"High school graduate\",\\n  \"BIRTH_PLACE\": \"US Born\",\\n  \"CITIZENSHIP\": \"US Citizen\",\\n  \"BIRTH_DETAIL\": \"State of residence\",\\n  \"STATE_NAME\": \"California\",\\n  \"STATE_ABBR\": \"CA\"\\n}'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing California personas:   0%|          | 0/1000 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.28s/it, est. speed input: 88.45 toks/s, output: 52.08 toks/s]\n",
      "Processing California personas:   0%|          | 1/1000 [00:05<1:28:05,  5.29s/it]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.50s/it, est. speed input: 83.18 toks/s, output: 52.06 toks/s]\n",
      "Processing California personas:   0%|          | 2/1000 [00:10<1:30:05,  5.42s/it]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.09s/it, est. speed input: 87.03 toks/s, output: 51.86 toks/s]\n",
      "Processing California personas:   0%|          | 3/1000 [00:15<1:27:37,  5.27s/it]\n",
      "\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [14:02<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts:   0%|          | 0/1 [11:21<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts:   0%|          | 0/1 [10:27<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "\n",
      "\n",
      "\n",
      "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.72s/it, est. speed input: 75.68 toks/s, output: 49.99 toks/s]\n",
      "Processing California personas:   0%|          | 4/1000 [00:21<1:30:33,  5.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing metadata for CA_persona_3.json. Using empty dictionary.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.32s/it, est. speed input: 77.90 toks/s, output: 51.93 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.97s/it, est. speed input: 86.35 toks/s, output: 52.34 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.06s/it, est. speed input: 86.77 toks/s, output: 52.18 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.42s/it, est. speed input: 66.60 toks/s, output: 52.21 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.19s/it, est. speed input: 68.06 toks/s, output: 53.21 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.92s/it, est. speed input: 71.57 toks/s, output: 52.86 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.27s/it, est. speed input: 68.66 toks/s, output: 52.53 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.19s/it, est. speed input: 68.19 toks/s, output: 52.01 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.90s/it, est. speed input: 70.88 toks/s, output: 52.91 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.46s/it, est. speed input: 64.67 toks/s, output: 52.58 toks/s]\n",
      "Processing California personas:   1%|▏         | 14/1000 [01:13<1:25:43,  5.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing metadata for CA_persona_13.json. Using empty dictionary.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.64s/it, est. speed input: 62.66 toks/s, output: 52.19 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.46s/it, est. speed input: 64.19 toks/s, output: 53.00 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.22s/it, est. speed input: 67.42 toks/s, output: 52.09 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.05s/it, est. speed input: 70.16 toks/s, output: 52.12 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.82s/it, est. speed input: 60.66 toks/s, output: 52.07 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.69s/it, est. speed input: 75.73 toks/s, output: 52.48 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.50s/it, est. speed input: 64.57 toks/s, output: 52.20 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.88s/it, est. speed input: 61.07 toks/s, output: 53.07 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.51s/it, est. speed input: 64.06 toks/s, output: 52.45 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.40s/it, est. speed input: 65.96 toks/s, output: 51.88 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.64s/it, est. speed input: 61.72 toks/s, output: 52.86 toks/s]\n",
      "Processing California personas:   2%|▎         | 25/1000 [02:13<1:29:42,  5.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing metadata for CA_persona_24.json. Using empty dictionary.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.13s/it, est. speed input: 68.91 toks/s, output: 52.12 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.86s/it, est. speed input: 73.51 toks/s, output: 52.10 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.54s/it, est. speed input: 64.88 toks/s, output: 52.95 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.16s/it, est. speed input: 67.90 toks/s, output: 52.77 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.05s/it, est. speed input: 87.08 toks/s, output: 51.85 toks/s]\n",
      "Processing California personas:   3%|▎         | 30/1000 [02:39<1:24:21,  5.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing metadata for CA_persona_29.json. Using empty dictionary.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.56s/it, est. speed input: 62.61 toks/s, output: 52.17 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.82s/it, est. speed input: 72.47 toks/s, output: 52.12 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.85s/it, est. speed input: 75.01 toks/s, output: 52.14 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.27s/it, est. speed input: 66.07 toks/s, output: 53.35 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.93s/it, est. speed input: 72.58 toks/s, output: 52.91 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.25s/it, est. speed input: 67.58 toks/s, output: 52.54 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.09s/it, est. speed input: 70.96 toks/s, output: 52.09 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.27s/it, est. speed input: 67.91 toks/s, output: 51.97 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.95s/it, est. speed input: 71.71 toks/s, output: 52.72 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.12s/it, est. speed input: 70.03 toks/s, output: 52.23 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.07s/it, est. speed input: 69.28 toks/s, output: 51.91 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.35s/it, est. speed input: 66.02 toks/s, output: 52.93 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.13s/it, est. speed input: 69.37 toks/s, output: 52.22 toks/s]\n",
      "Processing California personas:   4%|▍         | 43/1000 [03:45<1:22:26,  5.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing metadata for CA_persona_42.json. Using empty dictionary.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.15s/it, est. speed input: 68.04 toks/s, output: 51.90 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.20s/it, est. speed input: 68.26 toks/s, output: 53.07 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.30s/it, est. speed input: 83.57 toks/s, output: 52.44 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.36s/it, est. speed input: 66.25 toks/s, output: 52.06 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.90s/it, est. speed input: 70.61 toks/s, output: 53.06 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.32s/it, est. speed input: 65.99 toks/s, output: 52.83 toks/s]\n",
      "Processing California personas:   5%|▍         | 49/1000 [04:17<1:22:34,  5.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing metadata for CA_persona_48.json. Using empty dictionary.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.74s/it, est. speed input: 61.68 toks/s, output: 52.27 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.72s/it, est. speed input: 62.47 toks/s, output: 52.14 toks/s]\n",
      "Processing California personas:   5%|▌         | 51/1000 [04:28<1:26:38,  5.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing metadata for CA_persona_50.json. Using empty dictionary.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.79s/it, est. speed input: 73.28 toks/s, output: 52.41 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.46s/it, est. speed input: 64.50 toks/s, output: 52.22 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.99s/it, est. speed input: 71.23 toks/s, output: 53.17 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.07s/it, est. speed input: 68.31 toks/s, output: 52.72 toks/s]\n",
      "Processing California personas:   6%|▌         | 55/1000 [04:49<1:21:40,  5.19s/it]"
     ]
    }
   ],
   "source": [
    "import us\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_state_personas(state):\n",
    "    base_dir = f\"/user/al4263/Simulate/Persona/data/persona_meta/{state.abbr}\"\n",
    "    files = [f for f in os.listdir(base_dir) if f.endswith('.json')]\n",
    "    \n",
    "    for i in tqdm(range(1000), desc=f\"Processing {state.name} personas\"):\n",
    "        file_name = f\"{state.abbr}_persona_{i}.json\"\n",
    "        file_path = os.path.join(base_dir, file_name)\n",
    "        \n",
    "        with open(file_path, \"r\") as f:\n",
    "            metadata = json.load(f)\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": f\"Based on the following verified metadata, please generate a detailed persona description:\\n\\n{json.dumps(metadata, indent=2)}\"}\n",
    "        ]\n",
    "        prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "        output = generate_text(llm, prompt)\n",
    "        metadata, persona = extract_metadata_and_persona(output)    \n",
    "        \n",
    "        save_metadata_and_persona(state, i, metadata, persona)\n",
    "\n",
    "# Process personas for all states\n",
    "# for state in tqdm(us.states.STATES, desc=\"Processing states\"):\n",
    "#     process_state_personas(state)\n",
    "\n",
    "\n",
    "# print(\"Persona generation complete for all states.\")\n",
    "# do it for CA first\n",
    "process_state_personas(us.states.CA)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
